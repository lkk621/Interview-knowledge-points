1绪论
1.1研究背景及意义
随着我国基础设施建设的逐步完善，人民生活质量和水平逐步提升，这对城市信息化提出了更高的要求。加之近些年来“平安城市”、“智慧城市”等词逐渐出现在政府工作报告中，“平安城市”、“智慧城市”的概念越来越深入人心。而随着科学技术的不断发展，视频监控系统在我国各个地方逐步普及，尤其在交通路口、机场，火车站、商场、公园等人员复杂的公共场所。通过安装监控摄像头，可以第一时间获取各种突发事件的信息，以便于及时处理。例如走失人员搜索，犯罪嫌疑人追踪等。而传统的做法是工作人员坐在电脑面前对摄像头拍到的图片进行一帧一帧的查看，如果事件发生在一个有上百个摄像头的大型公共场所，这么做显然要耗费大量人力，并且人在长时间观察监控视频时，很难一直保持注意力高度集中，更不可能一天24小时都在工作。而随着各种互联网+应用的发展，智慧安防也逐步发展起来。智慧安防通过把传统摄像头、互联网及当前火热的人工智能技术结合起来，在面对各种复杂的场景时，显示了其强大的信息处理能力，为平安城市、智慧城市的建设提供了更好的解决方案。
随着数字图像处理、大数据、人工智能等技术的快速发展及其在安防领域的应用，智慧安防技术也得到了快速发展。与传统安防技术相比，智慧安防技术能对海量数据进行更高效的处理和更精确的分析，从而在各种复杂的场景下更好的协助工作人员。而行人重识别技术则是一个智慧安防系统里面最核心的技术之一，其任务是在给定的大量图像数据里面自动搜索出某个人物的其他图像，并且这些图像都是由不同摄像头以不同视角拍到的。该项技术除了应用在安防领域外，还可以应用到了商业领域。比如在某个超市里面，可以根据摄像头拍到的图像，自动分析出一个顾客进店之后在哪些商品前逗留了多长时间，据此分析出顾客对哪些商品感兴趣，以辅助商业决策。
学术界又把行人重识别称为行人再识别（Person Re-identification），简称ReID。作为图像检索的一个子问题，可以应用到无法进行人脸识别的一些场景。例如现在的监控摄像头大都使用广角镜头，以追求拍摄到更大的范围，这会导致单位监控区域的像素变少，拍摄到的人脸模糊，不足以进行人脸识别；而且有时还会因为拍摄角度的原因，导致只能拍到头顶、侧脸或者背面；还有的情况下要追踪的目标戴有口罩、面具等也无法进行人脸识别。在这些场景下，行人重识别技术可以作为作为人脸识别技术的一个扩充，以实现目标跟踪在时间上和空间上的连续性。
来自不同拍摄设备的同一个人的图像会因为角度的不同导致图像中行人姿态不一样，或者距离远导致分辨率低，又或者人物的某部分被遮挡了，这些都会给行人重识别带来不同程度的挑战，如图1所示。其中遮挡问题带来的挑战尤为严重，这是因为：（1）遮挡导致行人特征减少。目前的行人重识别任务都是靠提取有鉴别力的特征来进行行人识别的，算法的准确性在很大程度上取决于提取到的特征是否具有足够的鉴别力。（2）不同行人可能被相似的遮挡物遮挡，在进行行人重识别任务时遮挡会引入错误的信息，导致识别错误。
由于这些挑战的存在，对遮挡行人重识别问题的研究可以拓宽行人重识别技术的应用场景，应对更复杂的情况，因此具有很高的研究价值和实际意义。

1.2遮挡行人重识别问题研究现状
行人重识别问题最早被称为跨区域目标匹配。在2006年的计算机顶会CVPR上，Gheissari等首次提出了行人重识别（Person Re-identification）的概念，当时是把图像的颜色及边缘直方图作为图像特征进行人物匹配的。传统的人物重识别方法分为图像特征提取和相似性度量两个阶段。随着近年来深度学习的发展及在计算机视觉领域的广泛应用，越来越多的研究学者借助深度学习模型来实现行人重识别任务。2014年，在Yi等人发表的论文中，首次将深度学习应用到行人重识别领域，联合学习了特征提取和相似性度量两个过程，并且取得了较好的成果。本节将分别从手工特征设计和基于深度学习的行人重识别方法介绍国内外研究动态。
1.2.1基于手工特征的行人重识别
在深度学习发展起来以前，行人重识别任务都是基于人工提取特征的方法来做的。常用的特征有颜色、形状、纹理等表层特征。颜色特征是行人图像中最具表现力的特征之一，一些学者提出了基于颜色的特征描述。Farenzena等最早通过图像颜色直方图和颜色空域排序提取特征，同时还利用了空间位置信息来实现行人重识别。但颜色信息易受到光照和遮挡等因素影响，提取到的特征鲁棒性低。Gray等人利用Adaboost算法，将行人图像分成若干水平条纹，在RGB，YCbRC和HS颜色空间中计算颜色直方图，并用具有旋转不变性的Gabor滤波器和Schmid滤波器计算纹理直方图，提取了颜色特征和纹理特征，相比较单独使用颜色特征，提高了重识别的准确率。Sebastian等人将图像划分为一个个三角形区域，提取了局部特征。Zeng等[7]提出融合了空间直方图和协方差的行人特征描述符，空间特征来自身体不同区域的颜色通道，并以颜色和强度梯度作为像素特征。Matsukawa等人提出了一种基于像素层次分布的特征表示方法，将图像分成多个水平区域，再把每个区域细分为多个局部块，并用高斯分布建模局部块的颜色特征和纹理特征，即把各个水平区域建模为一组的多个高斯分布，其中每个高斯分布作为局部区域的特征，最后再用一个高斯分布来描述这组高斯分布的特性，以此得到图像的特征。Liao等人提出了LOMO（Local Maximal Occurrence）特征。首先把图像经过Retinex多尺度变换，可以减少光照变化对图像的影响。然后用一个小窗口在图像上滑动，计算每个窗口的HSV颜色直方图和纹理直方图得到局部特征，并融合所有子窗口在同一水平位置的特征，以此得到全局特征，这样可以减小视角变化带来的影响。由于减小了光照、视角和分辨率的影响，又兼顾了特征提取效率和准确率，该方法超过了同年大部分深度学习方法。
1.2.2基于深度学习的行人重识别
随着深度学习的兴起及卷积神经网络在图像处理领域的强大优势，以及基于传统手工特征的行人重识别方法性能提升遇到瓶颈，很多研究学者转向借助深度模型来解决行人重识别问题，并且随着研究的深入，很多提出的方法性能都优于基于传统手工特征的方法。基于传统手工特征的方法把特征提取和相似性度量分开独立进行，而深度学习模型协同优化特征提取和度量学习两个模块，以端到端的方式训练网络，简化了行人重识别任务，并且提高了识别准确率。
（2）基于局部特征的ReID方法
以前的行人重识别研究中人们大都关注全局特征，就是图像输入之后简单的经过一些卷积层、池化层、全连接层和激活后得到图像的特征向量。随着研究的深入，基于全局特征的重识别任务性能也遇到了瓶颈，慢慢就有研究学者开始关注局部特征。Rahul[12]等人最早将图片沿水平方向切分成若干块，再分别提取每一块的特征，如图所示。这种方法要求输入图像严格对齐，否则会出现一张图像上的头部和另外一张图像上的身体部位进行比对的情况，对模型训练来说这显然是致命的。但不可能所有的输入图像中行人都是对齐的，为了解决这个问题，Zheng[13]等人提出了人体姿态和骨架关键点模型。模型把行人分为14个关键点，再把这14个关键点划分三个区域，把三个区域对齐以矫正图像，然后把矫正后的图像和原始图像作为模型输入，最后输出包含全局和局部信息的特征向量。而在Zhao[14]等人的工作中也用到了关键点模型提取特征，但提出的Spindle Net没有追求局部区域的对齐，而是用关键点信息取出图像的ROI（Region of interest），具体如图，先用关键点模型提取出14个人体关键点，再由关键点得到7个ROI，分别是四肢的四个区域和头、上身、下身。再把ROI和原始图像输入模型提取特征。ROI得到的是7个局部特征，原图像得到的是一个全局特征，这些特征整合之后作为行人重识别任务的特征。为了避免引入额外的人体姿态估计和关键点模型，Zhang[16]等提出了基于距离的自动对齐模型，如图所示，该方法可以将图片中的行人自动对齐，之后把图像切分成若干块，分别提取各自的局部特征。
（3）基于度量学习的ReID方法
表征学习让网络训练特征提取能力，旨在让提取出来的特征更具区分力，而度量学习则是通过网络学习出两张图片的相似度。通过最大化类间距离和最小化类内距离来让模型提高识别能力，具体的，让属于同一个身份的不同图像之间的相似度变大，让属于不同身份的不同图像间的相似度变小，且至少要比同一个身份的不同图像之间的相似度小一个margin。目前提出的度量学习损失函数有对比损失(Contrastive loss) [5]、三元组损失(Triplet loss)[6-8]、 四元组损失(Quadruplet loss)[9]、难样本采样三元组损失(TriHard loss)[10]、边界挖掘损失(MSML)[11]。这些损失函数以欧氏距离或者余弦距离作为度量。孪生网络即是用对比损失训练的。输入的是一个图像对，标签x=1表示这两张图像属于同一个身份，x=0则表示输入的图像属于不同身份。损失函数为。
三元组损失是被用的最多的损失，其输入为三张图像别为Anchor、Positive、Negative。Positive和Anchor属于同一身份，Negative和Anchor属于不同身份。三张图像分为两个图像对，sitive和Anchor（正样本）、Negative和Anchor（负样本），之后输入到网络进行训练。网络会让正样本之间的距离变小，让负样本之间的距离变大，最后让模型可以更好的完成行人重识别任务。四元组损失是基于三元组损失提出来的，相比于三元组损失，四元组损失要求输入两张负样本图像，两个负样本属于不同身份。三元组损失只考虑正负样本间的相对距离，而四元组通过设定一大一小的margin，让网络能考虑到正负样本间的绝对距离，这样模型能学到更好的特征。难样本采样三元组损失是Alexander[10]等人提出的另一种改进的三元组损失，相比于传统的三元组损失，难样本采样方法并不是直接随机从数据集中选取Positive和Negative，而是从输入的一个batch中挑选出一张与Anchor最不相似的但属于同一身份的图像作为Positive，再选出一张与Anchor最相似的但不属于同一身份的图像作为Negative。这么做的出发点是属于同一身份的图像之间的相似度原本就比属于不同身份的图像间的相似度高，所以训练出来的网络泛化能力低，而难样本采样三元组损失训练出来的网络无疑具有更强的泛化能力。
1.2.3遮挡行人重识别
大多数研究学者都假设能拍到行人完整的图像，实际上这是一种较为理想的情况，现实场景中会遇到各种遮挡问题，而目前提出的行人重识别方法在测试带有遮挡的数据集时性能急剧下降，这也是为什么现在行人重识别还只是停留在研究阶段，很难落地的原因。随着在行人重识别领域研究的深入，开始有学者关注遮挡问题。Zhang等人提出了模糊匹配模型，为了应对遮挡问题，模型把输入图像分为若干图像块，并引入了图像块相似性评分机制，还利用了空间布局信息来让图像块尽可能对齐匹配。Kin等人研究的是部分行人图像，可以看成一种特殊的遮挡。他们的模型把人体划分为13个局部区域，对于具体的图像，在遮挡情况下，这13个局部区域会丢失其中一部分，匹配的时候仅针对两图像中共有的区域。He提出了另一种针对部分人重识别的方法，该方法分为FCN和DSR两部分。常规的行人重识别任务中，输入图像大小是相同的，而在部分行人重识别中图像大小不一，作者就在FCN部分删除了全连接层，只保留了卷积层和池化层，因为全连接层最后需要输出固定大小的特征向量。这样FCN就能根据输入图像的大小，输出大小可变的特征图。DSR部分通过重构特征来训练网络，具体地由FCN得到两张输入图像的特征向量X和Y，作者用Y中项的线性组合来重构X，重构矩阵w为一个稀疏矩阵，因为Y中只有很少一部分与重构X相关。通过最小化重构之后的同ID特征向量之间的距离，最大化不同ID特征向量之间的距离来训练模型。Miao等人提出了一种姿势引导特征对齐(Pose-Guided Feature Alignment，PGFA)的方法来解决行人重识别中的遮挡问题。该研究中作者重构了数据集，把原数据集中query部分遮挡图像占比由15%提升到了100%。为了提取关键点，作者通过COCO数据集标注的人体特征点数据训练一个人体特征点检测网络，利用姿态将有用信息从遮挡中分离出来。将遮挡图像输入网络，得到了未遮挡区域的关键点，并利用关键点坐标来生成注意力图，生成的注意力图指示了特定的身体部位是否遮挡，引导模型关注没有遮挡的区域，在匹配时，模型只会计算两张图像中没有遮挡区域的相似度。Zhuo等人提出了人体注意力框架(AFPB)以解决遮挡问题。该框架包括两部分，遮挡模拟器（Occlusion Simulator，OS）和多任务损失网络。OS是为了应对遮挡数据集不足的问题，在无遮挡的图像上随机生成遮挡块，增加了网络的训练数据。在多任务损失模块融合了识别损失和遮挡/非遮挡二分类损失，迫使网络不仅要区分行人身份，还要识别出一个样本是有遮挡的还是无遮挡的。实验结果表明该模型在遮挡行人重识别任务上的有效性。以上介绍了一些目前提出的针对遮挡行人重识别比较有效的方法，大部分都是先取出未遮挡的区域，然后针对未遮挡的区域提取特征，各方法的不同点在于如何更好的提取未遮挡区域的特征，使得到的特征更具鲁棒性和鉴别力。 
1.3本文主要内容
本文主要针对遮挡场景下的行人重识别问题，介绍一个针对公开数据集目前表现最好的方法，并在此基础上做了一些改进，并通过实验证明了改进是有效果的。
该方法主要包括三个模块，分别是一阶语义模块（Semantic，S）、高阶关系模块（Relation，R）和高阶人体拓扑模块（Human-Topology，T）。
r(k)={█(1   如果前n项包含和query相同身份的图像@0 其他)┤

1.4本文的组织结构
2相关背景知识介绍
2.1深度学习相关理论
2.1.1神经网络
神经网络是基于人大脑皮层神经元工作原理提出来的。一个神经元的结构通常包括一个轴突和多个树突，树突与其他神经元连接，接收来自其他神经元的信号，轴突尾部有许多轴突末梢和其他神经元的树突连接，可以传递信号给其他神经元。1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，提出了神经元模型MP。该神经元模型包含输入、输出和计算功能，分别类比了神经元的树突、轴突和细胞核的功能。
	单个神经元结构可表示为下图，
包含有3个输入，1个输出，以及2个计算功能。三个输入经过线性组合之后经过一个非线性函数得到一个输出。但在MP模型中，权重的值都是预先设置的，不能进行学习。1949年心理学家Hebb发现人脑神经元之间的连接强度是会变化的，科学家们开始考虑让模型自己学习权重参数，但受限于当时计算机的计算能力，神经网络并没有得到很好的发展。十年后计算科学家Rosenblatt提出了感知机模型，它实际上是一个由两层神经元组成的神经网络，是首个可以学习的人工神经网络。但这个感知机只能做简单的线性分类任务。科学家Minsky认为，如果将计算层增加到两层，可以解决非线性分类任务，但计算量过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。由于当时Minsky巨大的影响力和其对神经网络的悲观态度，让很多学者和实验室都放弃了神经网络的研究。
	直到1986年Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络计算量复杂的问题，才带来了神经网络的又一轮研究热潮。此时的神经网络是一个含有3层的网络结构，分别是输入层、中间层、输出层，中间层和输出层带有计算功能，可以进行非线性分类任务。随着研究的深入，神经网络的诸多问题又暴露出来了，尽管使用了BP算法，神经网络的训练仍然需要耗时很久，而且容易陷入局部最优解，使得神经网络的优化较为困难。到了90年代中期，由Vapnik等人发明的支持向量机（Support Vector Machines，SVM）算法诞生，SVM无需调参，效率高，且能找到全局最优解，基于这些优势，SVM迅速打败了神经网络算法，对神经网络的研究又一次陷入了冰期。
	2006年，Hinton在《Science》上发表论文，首次提出了“深度信念网络”的概念。深度信念网络有一个预训练的过程，这可以让网络中的权值找到一个接近最优解的值，之后再使用微调技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法取名为深度学习。随后，深度学习语言识别领域取得了很好的效果，之后，在图像识别领域也大展拳脚。直到现在，关于深度神经网络的研究与应用已经渗透到各行业了。

2.1.2卷积神经网络
上一小节讲述了神经网络的发展历程，而卷积神经网络（Convolutional Neural Networks, CNN）也是随着神经网络的发展而发展起来的。卷积神经网络是一种带有卷积结构的深度神经网络，它有三个关键操作：局部连接、权值共享、池化。局部连接的思想来源于最初生物学家对猫视觉皮层细胞的研究而提出来的感受野的概念，感受野对视觉输入空间的某个子区域非常敏感，而不是对整个输入。在CNN中，把各个神经元的局部信息综合起来就能得到全局信息。值共享是让不同神经元之间共享权值参数。这三个操作有效的减少了网络的参数个数，能有效缓解模型的过拟合。
如图是一个基本的卷积神经网络结构图。
对于一个图像分类任务的卷积神经网络一般包括如下几个层：
（1）	输入层（input layer）
图像输入之后在计算机中是以像素矩阵存储的，是一个三维向量，包括通道、长度、宽度三个维度，通道数可以看做是图像的深度。
（2）	卷积层
https://www.sumaarts.com/share/620.html
卷积操作是针对每个通道上的一个二维向量进行的。一个卷积层的作用可以看做是一次特征提取，或者滤波。假如输入一幅3*28*28的图像做图像识别任务，图像上的每个像素点并不都是有用的，我们可以通过卷积提取其中对识别任务影响大的像素信息，在前向传播过程中，会有很多个卷积层，而每一次卷积操作可以提取到更加抽象的信息。总的来说，卷积层的作用有a. 滤波或者说是特征提取，b. 降低参数数据量，这是因为卷积具有权值共享特性。
每一个卷积操作可以表示如下，
选取一个3*3或者5*5的卷积核在沿通道维度上的每一个二维矩阵上运算
（3）	池化层
池化层的作用主要是对特征图进行压缩，提取到主要特征，降低网络的计算复杂度。
（4）	全连接层（fully-connected layer，FC）
全连接层在整个卷积神经网络中起到“分类器”的作用。卷积层、池化层和激活层可以看作是是将原始数据映射到隐层特征空间，全连接层则是将学到的特征映射到样本标记空间。
（5）	激活层（activate layer）
信号在生物神经元上传递时，只有大于特定的阈值后面的神经元才会被激活。而CNN的激活函数也具有类似的作用，它是一个非线性函数。引入非线性激活函数的主要目的是增加网络的非线性。前面说过，神经网络实质上是在模拟一个从输入到输出的非线性映射，如果没有非线性激活函数，每一层输出都是上一层输出的线性组合，因此，无论神经网络有多少层，得到的输出都是输入的线性组合，层数的增加只不过是增加组合的复杂度，这就是原始的感知机模型。当加入了非线性激活函数，网络经过训练后就可以模拟出我们想要的映射。常用的激活函数有Sigmod、tanh、ReLU 、leaky ReLU等。
（6）	Softmax层
在一个分类任务中，我们希望最后的输出结果是一个概率，比如一张输入图片是猫的概率是多少，是狗的概率是多少，而且所有类别的的概率和为1。而Softmax函数为
可以看出，它可以把一个序列的值映射到[0,1]，且彼此之间的大小关系不变。
近年来，由于卷积神经网络在图像处理方面的巨大优势，已被广泛用于和计算机视觉相关的各个领域，如人脸识别、字符识别、目标跟踪等。除了计算机视觉外，卷积神经网络还被用于自然语言处理领域，包括语音识别、语义分析、对话系统等。在未来，CNN还会被应用到更多的领域，助力人工智能让生活更智慧。


2.2图卷积
2.3遮挡行人重识别评价指标
行人重识别被认为是图像检索的一个子问题，所以图像检索的评价指标也被用于评价行人重识别问题，其中最常用的有累计匹配特征（Cumulative Match Characteristics, CMC）曲线和平均准确度均值（mean Average Precision, mAP）。
	累计匹配特征曲线（CMC）
累计匹配特征曲线常用来评价人脸识别、指纹识别等计算机模式识别系统的性能。CMC曲线的横坐标为rank，取值为1,2,3…n，纵坐标为识别率。具体如下：
对于每一个给定的查询图像query，行人重识别系统在gallery中对所有图像按和query的相似度从大到小排序，
r(k)={█(1   如果前n项包含和query相同身份的图像@0 其他)┤
rank= 
其中N为待查询图像的总数，对于一个查询样本img1来说，如果模型返回的结果为img1、img2、img3、img4、img5…，此时rank-1的正确率为100%；rank-2的正确率也为100%；rank-5的正确率也为100%；如果模型返回的结果为img2、img1、img3、img4、img5…，此时rank-1的正确率为0%；rank-2的正确率为100%；rank-5的正确率也为100%；剩下的情况以此类推。所以在一个行人重识别系统中，rank-n即表示相似度最高的n张图像中包含待识别图像的平均概率。
	但是，该评价指标只适用于在gallery中只有一个query的情况，当gallery中有多个query时，在返回结果中，只是第一个query的位置对CMC曲线有影响，而忽略了剩余query的匹配情况，这显然不利于评价一个行人重识别算法的整体性能。
	平均准确度均值（mAP）
累计匹配特性曲线只能显示出算法识别出第一个query的能力，而剩下不易匹配出来的query的匹配能力对评估一个算法同样重要，mAP评价指标就可以弥补CMC的不足。在行人重识别任务中，准确度均值（Average Precision, AP）是衡量一个query的识别准确率，而平均准确度均值（mean Average Precision, mAP）是衡量模型在所有query上的识别准确率。其计算方法如下：
给定标签为1的待查询图像query，行人重识别系统在图像库中检后的返回结果为img1、img2、img3、img4…img10，其标签和准确率计算如下表所示。
该查询图像query的准确度均值为AP=(1+0.67+0.5+0.44+0.5)/5=0.62。在整个query集中的平均准确度的均值 mAP=(AP1+AP2+…+APn)/n。
相比于CMC，mAP能更加全面衡量行人重识别算法在数据集上的效果，它反映了待查询的人在数据库中的图片排在查询结果队列前面的程度，而不仅仅是首位命中。在行人重识别领域还有其他不太常见的评价方法，如召回率，以及准确率和召回率的调和平均数F-score。
本文中的方法在评价效果时采用了CMC，mAP，事实上，每年提出了的行人重识别方法都会选择以上两个评价指标来评价算法的好坏。

2.4遮挡行人重识别数据集
深度学习方法需要依赖大规模的数据集，数据集的规模和质量在一定程度上决定了一个模型的好坏。本节主要介绍行人重识别领域常用的数据集。
目前用于行人重识别任务的数据集可分为两类，一类是图片数据集，图片之间没有关联关系，有的数据虽然是从视频中截取出来的图像帧，但在训练的时候不会利用前后帧之间的关系，该类数据集有Market-1501、DukeMTMC-reID、MSMT17和CUHk等。还有一类是视频数据集，该类数据集行人图像帧之间有关联关系，该类数据集有PRID2011和MARS等。
Market-1501数据集是在清华校园拍摄并公开的，使用了5个高清摄像机和1个低清摄像机进行拍摄。该数据集一共包含1501个行人，每个行人都至少被两个摄像头拍摄到，其中751个行人的12936张图像被用作训练集，剩下的750个行人的图像作为测试集，测试集中3368张图像为query集，剩下19732张图像为gallery集。
DukeMTMC-reID数据集是 DukeMTMC 数据集用作行人重识别任务的一个子集，由杜克大学的研究学者拍摄制作。DukeMTMC 包含由8个不同摄像头的视频，DukeMTMC-reID数据集通过选取视频中的有行人的图像帧得到。共包括1812个行人的36411张图像，其中408 个人的图像只出现在一个摄像头下，1404个人的图像出现在大于两个摄像头下。从出现在两个摄像头下的行人中随机选择702个作为训练集，剩下的702人中选取每个摄像头拍摄的一张图像作为query，剩下的放进gallery，而只出现在一个摄像头中的408人的图像也被放到gallery作为干扰。
CUHK数据集是在香港中文大学拍摄的，包含CUHK01、CUHK02、CUHK03三个子数据集，CUHK01由一组相机拍摄，每个相机拍摄一个行人的两张不同图像，总共拍摄了971个行人的3884张图像。CUHK02是CUHK01的扩充，还包含了另外8个摄像头拍摄的7264张图片。CUHK03是最常用的数据集之一，由10个相机拍摄了1467个人的图像，其中1367个人的13132张图像作为训练集，100个人的965张图像作为gallery和query。
以上介绍的数据集大都是行人完整的数据集，用于一般的行人重识别任务，而对于遮挡行人重识别任务则需要用到带遮挡的数据集，下面介绍几个带遮挡的数据集。
Occluded-Duke数据集是Miao等人在他们的论文中通过在DukeMTMC-reID数据集中留下一些带遮挡的图像，并过滤掉一些重叠的图像得到的，包含15618个训练图像，17661个gallery图像和2210个query图像。
Occluded-ReID数据集由Zhuo等人用一台移动摄像机拍摄，由200名行人的2000张图像组成，其中每个行人包含5张全身图像和5张不同类型遮挡的图像。
Partial REID数据集制作规则和Occluded-ReID数据集一样，不过只包含60个人的600张图像。
Partial-iLIDS 数据集来自 iLIDS，拍摄地点在机场，行人的下半身经常被一些物体遮挡了。通过从这些图像中裁剪出非遮挡区域得到Partial-iLIDS数据集，包含了119个行人476 张图像。
以上介绍的遮挡行人重识别数据集规模有的太小了，不利于训练和测试，所以本文中的方法选用了数据规模较大的Occluded-Duke数据集和Occluded-ReID数据集做训练和测试。

2.5本章小结
3遮挡行人重识别研究
3.1问题描述与框架设计
3.2一阶语义
3.3高阶关系
3.4高阶人体拓扑结构
3.5本章小结
4实验
4.1xx

5总结与展望
5.1总结
5.2展望
